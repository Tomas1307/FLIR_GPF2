{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.05s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1030/1030 [32:47<00:00,  1.91s/it]\n",
      "100%|██████████| 492/492 [12:06<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing results...\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 154\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    153\u001b[0m     train_one_epoch(model, optimizer, train_loader)\n\u001b[1;32m--> 154\u001b[0m     \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoco\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m⏱ Tiempo por epoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(time\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m60\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m minutos\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    158\u001b[0m \u001b[38;5;66;03m# Guardar modelo\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[6], line 135\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(model, data_loader, coco_gt)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredictions.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    133\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(results, f, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 135\u001b[0m coco_dt \u001b[38;5;241m=\u001b[39m \u001b[43mcoco_gt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloadRes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpredictions.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    136\u001b[0m coco_eval \u001b[38;5;241m=\u001b[39m COCOeval(coco_gt, coco_dt, iouType\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbbox\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    137\u001b[0m coco_eval\u001b[38;5;241m.\u001b[39mevaluate()\n",
      "File \u001b[1;32mc:\\Users\\carra\\OneDrive\\Escritorio\\UNIVERSIDAD\\SEPTIMO\\IMAGENES Y VISION\\PROYECTO\\venv\\lib\\site-packages\\pycocotools\\coco.py:329\u001b[0m, in \u001b[0;36mCOCO.loadRes\u001b[1;34m(self, resFile)\u001b[0m\n\u001b[0;32m    326\u001b[0m annsImgIds \u001b[38;5;241m=\u001b[39m [ann[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_id\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m ann \u001b[38;5;129;01min\u001b[39;00m anns]\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mset\u001b[39m(annsImgIds) \u001b[38;5;241m==\u001b[39m (\u001b[38;5;28mset\u001b[39m(annsImgIds) \u001b[38;5;241m&\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgetImgIds())), \\\n\u001b[0;32m    328\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mResults do not correspond to current coco set\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 329\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcaption\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[43manns\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m:\n\u001b[0;32m    330\u001b[0m     imgIds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m([img[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m res\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m]]) \u001b[38;5;241m&\u001b[39m \u001b[38;5;28mset\u001b[39m([ann[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_id\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m ann \u001b[38;5;129;01min\u001b[39;00m anns])\n\u001b[0;32m    331\u001b[0m     res\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [img \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m res\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m img[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m imgIds]\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.transforms import functional as F\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Dataset personalizado\n",
    "class CocoDataset(Dataset):\n",
    "    def __init__(self, img_dir, ann_file, transforms=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.coco = COCO(ann_file)\n",
    "        self.ids = list(self.coco.imgs.keys())\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        coco = self.coco\n",
    "        img_id = self.ids[index]\n",
    "        ann_ids = coco.getAnnIds(imgIds=img_id)\n",
    "        anns = coco.loadAnns(ann_ids)\n",
    "\n",
    "        # Si no hay anotaciones, pasar a la siguiente imagen\n",
    "        if len(anns) == 0:\n",
    "            return self.__getitem__((index + 1) % len(self.ids))\n",
    "\n",
    "        path = coco.loadImgs(img_id)[0]['file_name']\n",
    "        img = Image.open(os.path.join(self.img_dir, path)).convert(\"RGB\")\n",
    "\n",
    "        boxes, labels, areas, iscrowd = [], [], [], []\n",
    "\n",
    "        for ann in anns:\n",
    "            x, y, w, h = ann['bbox']\n",
    "            if w <= 0 or h <= 0:\n",
    "                continue  # omitir cajas inválidas\n",
    "            boxes.append([x, y, x + w, y + h])\n",
    "            labels.append(ann['category_id'])\n",
    "            areas.append(ann['area'])\n",
    "            iscrowd.append(ann.get('iscrowd', 0))\n",
    "\n",
    "        if len(boxes) == 0:\n",
    "            return self.__getitem__((index + 1) % len(self.ids))\n",
    "\n",
    "        target = {\n",
    "            'boxes': torch.tensor(boxes, dtype=torch.float32),\n",
    "            'labels': torch.tensor(labels, dtype=torch.int64),\n",
    "            'image_id': torch.tensor([img_id]),\n",
    "            'area': torch.tensor(areas, dtype=torch.float32),\n",
    "            'iscrowd': torch.tensor(iscrowd, dtype=torch.int64)\n",
    "        }\n",
    "\n",
    "        if self.transforms:\n",
    "            img = self.transforms(img)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "# Transforms\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((512, 512)),\n",
    "    torchvision.transforms.ToTensor()\n",
    "])\n",
    "\n",
    "\n",
    "# Carga de datos\n",
    "train_dataset = CocoDataset(\"modelo_yolov11_dataset_completo/train/images\", \"modelo_yolov11_dataset_completo/train/train_coco.json\", transform)\n",
    "val_dataset = CocoDataset(\"modelo_yolov11_dataset_completo/val/images\", \"modelo_yolov11_dataset_completo/val/val_coco.json\", transform)\n",
    "\n",
    "train_subset = torch.utils.data.Subset(train_dataset, list(range(0, len(train_dataset), 4)))  # 25% del dataset\n",
    "train_loader = DataLoader(train_subset, batch_size=2, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
    "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "##train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
    "##val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "# Modelo\n",
    "model = fasterrcnn_resnet50_fpn(num_classes=len(train_dataset.coco.getCatIds()) + 1)\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# Entrenamiento\n",
    "def train_one_epoch(model, optimizer, data_loader):\n",
    "    model.train()\n",
    "    for images, targets in tqdm(data_loader):\n",
    "        images = list(img.to(device) for img in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Validación\n",
    "def evaluate(model, data_loader, coco_gt):\n",
    "    model.eval()\n",
    "    results = []\n",
    "    for images, targets in tqdm(data_loader):\n",
    "        images = list(img.to(device) for img in images)\n",
    "        outputs = model(images)\n",
    "        for target, output in zip(targets, outputs):\n",
    "            boxes = output['boxes'].cpu().detach().numpy()\n",
    "            scores = output['scores'].cpu().detach().numpy()\n",
    "            labels = output['labels'].cpu().detach().numpy()\n",
    "            image_id = int(target['image_id'])\n",
    "\n",
    "            for box, score, label in zip(boxes, scores, labels):\n",
    "                x_min, y_min, x_max, y_max = box\n",
    "                results.append({\n",
    "                    \"image_id\": image_id,\n",
    "                    \"category_id\": int(label),\n",
    "                    \"bbox\": [x_min, y_min, x_max - x_min, y_max - y_min],\n",
    "                    \"score\": float(score)\n",
    "                })\n",
    "\n",
    "    # Guardar predicciones temporales\n",
    "    with open(\"predictions.json\", \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "\n",
    "    coco_dt = coco_gt.loadRes(\"predictions.json\")\n",
    "    coco_eval = COCOeval(coco_gt, coco_dt, iouType='bbox')\n",
    "    coco_eval.evaluate()\n",
    "    coco_eval.accumulate()\n",
    "    coco_eval.summarize()\n",
    "\n",
    "    # Métricas por clase\n",
    "    cat_ids = coco_gt.getCatIds()\n",
    "    for i, cat_id in enumerate(cat_ids):\n",
    "        print(f\"Clase: {coco_gt.loadCats(cat_id)[0]['name']}\")\n",
    "        precision = coco_eval.eval['precision'][:, i, :, 0, -1]\n",
    "        recall = coco_eval.eval['recall'][:, i, 0, 0, -1]\n",
    "        print(f\"  Precision media: {np.mean(precision[precision > -1]):.4f}\")\n",
    "        print(f\"  Recall media:    {np.mean(recall[recall > -1]):.4f}\")\n",
    "\n",
    "# Entrenamiento por epochs\n",
    "for epoch in range(2):\n",
    "    print(f\"\\nEpoch {epoch+1}\")\n",
    "    train_one_epoch(model, optimizer, train_loader)\n",
    "    evaluate(model, val_loader, val_dataset.coco)\n",
    "    print(f\"⏱ Tiempo por epoch: {(time.time() - start)/60:.2f} minutos\")\n",
    "\n",
    "\n",
    "# Guardar modelo\n",
    "torch.save(model.state_dict(), \"fasterrcnn_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagen 0\n",
      "Cajas: tensor([[1273.2336,  458.2143, 1462.0944,  639.3076]])\n",
      "Etiquetas: tensor([0])\n",
      "Imagen 1\n",
      "Cajas: tensor([[1273.2336,  458.2143, 1462.0944,  639.3076]])\n",
      "Etiquetas: tensor([0])\n",
      "Imagen 2\n",
      "Cajas: tensor([[1273.2336,  458.2143, 1462.0944,  639.3076]])\n",
      "Etiquetas: tensor([0])\n",
      "Imagen 3\n",
      "Cajas: tensor([[1273.2336,  458.2143, 1462.0944,  639.3076]])\n",
      "Etiquetas: tensor([0])\n",
      "Imagen 4\n",
      "Cajas: tensor([[1273.2336,  458.2143, 1462.0944,  639.3076]])\n",
      "Etiquetas: tensor([0])\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    img, target = train_dataset[i]\n",
    "    print(f\"Imagen {i}\")\n",
    "    print(\"Cajas:\", target[\"boxes\"])\n",
    "    print(\"Etiquetas:\", target[\"labels\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
