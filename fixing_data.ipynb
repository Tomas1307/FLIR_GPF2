{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a06860f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting reorganization for: modelo_yolov11_dataset_completo ---\n",
      "Source: modelo_yolov11_dataset_completo\n",
      "Destination: fixed\\modelo_yolov11_dataset_completo_reorganized\n",
      "Found 107 original validation images from 'data\\Imagenes\\val'.\n",
      "Found 372 original test images from 'data\\Imagenes\\test'.\n",
      "\n",
      "Created new dataset structure in fixed\\modelo_yolov11_dataset_completo_reorganized\n",
      "\n",
      "Processing files from: modelo_yolov11_dataset_completo\\train\\images\n",
      "\n",
      "Processing files from: modelo_yolov11_dataset_completo\\val\\images\n",
      "\n",
      "Processing files from: modelo_yolov11_dataset_completo\\test\\images\n",
      "\n",
      "--- Reorganization for modelo_yolov11_dataset_completo Complete ---\n",
      "Total images moved to new train: 11215\n",
      "Total images moved to new val: 91\n",
      "Total images moved to new test: 294\n",
      "New organized dataset is in: fixed\\modelo_yolov11_dataset_completo_reorganized\n",
      "Remember to update your YOLO config (data.yaml) to point to this new structure.\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Starting reorganization for: modelo_yolov11_dataset_filtrado ---\n",
      "Source: modelo_yolov11_dataset_filtrado\n",
      "Destination: fixed\\modelo_yolov11_dataset_filtrado_reorganized\n",
      "Found 107 original validation images from 'data\\Imagenes\\val'.\n",
      "Found 372 original test images from 'data\\Imagenes\\test'.\n",
      "\n",
      "Created new dataset structure in fixed\\modelo_yolov11_dataset_filtrado_reorganized\n",
      "\n",
      "Processing files from: modelo_yolov11_dataset_filtrado\\train\\images\n",
      "\n",
      "Processing files from: modelo_yolov11_dataset_filtrado\\val\\images\n",
      "\n",
      "Processing files from: modelo_yolov11_dataset_filtrado\\test\\images\n",
      "\n",
      "--- Reorganization for modelo_yolov11_dataset_filtrado Complete ---\n",
      "Total images moved to new train: 11230\n",
      "Total images moved to new val: 81\n",
      "Total images moved to new test: 299\n",
      "New organized dataset is in: fixed\\modelo_yolov11_dataset_filtrado_reorganized\n",
      "Remember to update your YOLO config (data.yaml) to point to this new structure.\n",
      "--------------------------------------------------\n",
      "\n",
      "All reorganization tasks finished. Check the 'fixed' directory.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "def reorganize_dataset(generated_dataset_name: str, original_data_root: Path, output_base_root: Path):\n",
    "    \"\"\"\n",
    "    Reorganizes a generated dataset to ensure only original images are in val/test splits\n",
    "    and all augmented + original train images are in the train split.\n",
    "\n",
    "    Args:\n",
    "        generated_dataset_name (str): The name of the dataset to reorganize (e.g., \"modelo_yolov11_dataset_completo\").\n",
    "        original_data_root (Path): The root path of your original (unaugmented) data.\n",
    "                                   (e.g., Path(\"data\"))\n",
    "        output_base_root (Path): The base directory where the reorganized dataset will be saved.\n",
    "                                 (e.g., Path(\"fixed\"))\n",
    "    \"\"\"\n",
    "    GENERATED_DATASET_ROOT = Path(generated_dataset_name)\n",
    "    OUTPUT_DATASET_ROOT = output_base_root / f\"{generated_dataset_name}_reorganized\"\n",
    "\n",
    "    print(f\"\\n--- Starting reorganization for: {generated_dataset_name} ---\")\n",
    "    print(f\"Source: {GENERATED_DATASET_ROOT}\")\n",
    "    print(f\"Destination: {OUTPUT_DATASET_ROOT}\")\n",
    "\n",
    "    # --- Configuration for original data paths ---\n",
    "    ORIGINAL_IMG_DIR = original_data_root / \"Imagenes\"\n",
    "    ORIGINAL_LBL_DIR = original_data_root / \"Etiquetas\"\n",
    "\n",
    "    # --- Step 1: Identify Original Validation and Test Images ---\n",
    "    original_val_stems = set()\n",
    "    original_test_stems = set()\n",
    "\n",
    "    # Populate original_val_stems\n",
    "    original_val_img_path = ORIGINAL_IMG_DIR / \"val\"\n",
    "    if original_val_img_path.exists():\n",
    "        for img_file in original_val_img_path.glob(\"*.jpg\"):\n",
    "            original_val_stems.add(img_file.stem)\n",
    "        for img_file in original_val_img_path.glob(\"*.png\"):\n",
    "            original_val_stems.add(img_file.stem)\n",
    "    else:\n",
    "        print(f\"Warning: Original validation image path not found: {original_val_img_path}\")\n",
    "\n",
    "    # Populate original_test_stems\n",
    "    original_test_img_path = ORIGINAL_IMG_DIR / \"test\"\n",
    "    if original_test_img_path.exists():\n",
    "        for img_file in original_test_img_path.glob(\"*.jpg\"):\n",
    "            original_test_stems.add(img_file.stem)\n",
    "        for img_file in original_test_img_path.glob(\"*.png\"):\n",
    "            original_test_stems.add(img_file.stem)\n",
    "    else:\n",
    "        print(f\"Warning: Original test image path not found: {original_test_img_path}\")\n",
    "\n",
    "    print(f\"Found {len(original_val_stems)} original validation images from '{original_val_img_path}'.\")\n",
    "    print(f\"Found {len(original_test_stems)} original test images from '{original_test_img_path}'.\")\n",
    "\n",
    "    # --- Step 2: Prepare New Output Directories ---\n",
    "    new_train_img_dir = OUTPUT_DATASET_ROOT / \"train\" / \"images\"\n",
    "    new_train_lbl_dir = OUTPUT_DATASET_ROOT / \"train\" / \"labels\"\n",
    "    new_val_img_dir = OUTPUT_DATASET_ROOT / \"val\" / \"images\"\n",
    "    new_val_lbl_dir = OUTPUT_DATASET_ROOT / \"val\" / \"labels\"\n",
    "    new_test_img_dir = OUTPUT_DATASET_ROOT / \"test\" / \"images\"\n",
    "    new_test_lbl_dir = OUTPUT_DATASET_ROOT / \"test\" / \"labels\"\n",
    "\n",
    "    # Create all necessary directories\n",
    "    new_train_img_dir.mkdir(parents=True, exist_ok=True)\n",
    "    new_train_lbl_dir.mkdir(parents=True, exist_ok=True)\n",
    "    new_val_img_dir.mkdir(parents=True, exist_ok=True)\n",
    "    new_val_lbl_dir.mkdir(parents=True, exist_ok=True)\n",
    "    new_test_img_dir.mkdir(parents=True, exist_ok=True)\n",
    "    new_test_lbl_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(f\"\\nCreated new dataset structure in {OUTPUT_DATASET_ROOT}\")\n",
    "\n",
    "    # --- Step 3: Reorganize Files ---\n",
    "    moved_counts = {'train': 0, 'val': 0, 'test': 0}\n",
    "\n",
    "    # Iterate through the images and labels in the *GENERATED* dataset\n",
    "    # We need to check all original generated splits (train, val, test) because augmented data could be in any of them\n",
    "    for current_split_name in [\"train\", \"val\", \"test\"]:\n",
    "        current_img_dir = GENERATED_DATASET_ROOT / current_split_name / \"images\"\n",
    "        current_lbl_dir = GENERATED_DATASET_ROOT / current_split_name / \"labels\"\n",
    "\n",
    "        if not current_img_dir.exists():\n",
    "            print(f\"Warning: Skipping {current_img_dir}, path does not exist.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nProcessing files from: {current_img_dir}\")\n",
    "\n",
    "        # Use os.scandir for potentially better performance with large directories\n",
    "        with os.scandir(current_img_dir) as entries:\n",
    "            for entry in entries:\n",
    "                if entry.is_file() and (entry.name.lower().endswith('.jpg') or entry.name.lower().endswith('.png')):\n",
    "                    img_file = entry.name\n",
    "                    img_path = Path(entry.path) # Use Path for easier manipulation\n",
    "                    lbl_path = current_lbl_dir / (Path(img_file).stem + \".txt\")\n",
    "\n",
    "                    target_split = \"train\" # Default target is 'train'\n",
    "\n",
    "                    # Determine if it's an original validation or test image\n",
    "                    if Path(img_file).stem in original_val_stems:\n",
    "                        target_split = \"val\"\n",
    "                    elif Path(img_file).stem in original_test_stems:\n",
    "                        target_split = \"test\"\n",
    "\n",
    "                    # Define destination paths\n",
    "                    dest_img_path = OUTPUT_DATASET_ROOT / target_split / \"images\" / img_file\n",
    "                    dest_lbl_path = OUTPUT_DATASET_ROOT / target_split / \"labels\" / (Path(img_file).stem + \".txt\")\n",
    "\n",
    "                    # Move the image and label file\n",
    "                    try:\n",
    "                        # Only move if the destination doesn't already have it\n",
    "                        # This prevents issues if an original val/test image existed in multiple generated splits\n",
    "                        if not dest_img_path.exists():\n",
    "                            shutil.move(str(img_path), str(dest_img_path))\n",
    "                            if lbl_path.exists():\n",
    "                                shutil.move(str(lbl_path), str(dest_lbl_path))\n",
    "                            else:\n",
    "                                # If no label file, create an empty one in the destination\n",
    "                                Path(dest_lbl_path).touch()\n",
    "                            moved_counts[target_split] += 1\n",
    "                        # else:\n",
    "                        #     print(f\"Skipping {img_file} as it already exists in {dest_img_path.parent}.\") # Uncomment for verbose info on skips\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error moving {img_file} to {target_split}: {e}\")\n",
    "                elif entry.is_dir():\n",
    "                    print(f\"Skipping directory: {entry.path}\")\n",
    "\n",
    "\n",
    "    print(f\"\\n--- Reorganization for {generated_dataset_name} Complete ---\")\n",
    "    print(f\"Total images moved to new train: {moved_counts['train']}\")\n",
    "    print(f\"Total images moved to new val: {moved_counts['val']}\")\n",
    "    print(f\"Total images moved to new test: {moved_counts['test']}\")\n",
    "    print(f\"New organized dataset is in: {OUTPUT_DATASET_ROOT}\")\n",
    "    print(\"Remember to update your YOLO config (data.yaml) to point to this new structure.\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "\n",
    "\n",
    "ORIGINAL_DATA_ROOT = Path(\"data\")\n",
    "\n",
    "FIXED_OUTPUT_BASE = Path(\"fixed\")\n",
    "FIXED_OUTPUT_BASE.mkdir(exist_ok=True)\n",
    "\n",
    "# Run for modelo_yolov11_dataset_completo\n",
    "reorganize_dataset(\"modelo_yolov11_dataset_completo\", ORIGINAL_DATA_ROOT, FIXED_OUTPUT_BASE)\n",
    "\n",
    "# Run for modelo_yolov11_dataset_filtrado\n",
    "reorganize_dataset(\"modelo_yolov11_dataset_filtrado\", ORIGINAL_DATA_ROOT, FIXED_OUTPUT_BASE)\n",
    "\n",
    "print(\"\\nAll reorganization tasks finished. Check the 'fixed' directory.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
